{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 8\n",
      "[[   0.            0.            0.            0.            0.\n",
      "     0.            0.            0.            0.        ]\n",
      " [   0.           65.77308407   67.13647421   77.84605      79.84451583\n",
      "    72.47511769 -100.            0.          100.        ]\n",
      " [   0.           55.88294346 -100.           70.30818136   81.34440225\n",
      "    83.04847989   84.88054612   96.87232244   98.71875987]\n",
      " [   0.           54.92298013   50.47656297   59.66641187    0.\n",
      "    80.95826449    0.           97.04482865   98.72729893]\n",
      " [  53.50968756   54.14557214    0.         -100.         -100.\n",
      "    61.77980767 -100.           88.22035599  100.        ]\n",
      " [   0.           52.50402036   43.9359876    51.09137525   61.00715483\n",
      "    71.78642614   73.94661407   85.18458536   97.57257319]\n",
      " [   0.           43.77254574 -100.            0.            0.\n",
      "    70.35142939    0.         -100.           88.40593622]\n",
      " [   0.           47.95296148   48.76871928   58.14735126   59.39003194\n",
      "    60.1688947  -100.            0.          100.        ]\n",
      " [   0.            0.            0.            0.            0.\n",
      "     0.            0.            0.            0.        ]]\n",
      "[[1 1 1 1 1 1 1 1 1]\n",
      " [1 3 3 3 4 4 1 1 1]\n",
      " [1 2 1 3 3 3 3 3 2]\n",
      " [1 2 3 2 1 2 1 3 4]\n",
      " [3 2 1 1 1 2 1 3 1]\n",
      " [1 2 1 3 3 3 3 3 2]\n",
      " [1 2 1 1 1 2 1 1 4]\n",
      " [1 3 3 3 3 2 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Step 1: Read files\n",
    "'''\n",
    "Rewards = 81-d column array\n",
    "Probs = dict{action : {s:[(s1', p1),...]}}\n",
    "'''\n",
    "\n",
    "REWARD_FILE = 'rewards.txt'\n",
    "P_FILE = 'prob_a.txt'\n",
    "\n",
    "def read_rewards(fname):\n",
    "    with open(REWARD_FILE) as f:\n",
    "        return np.mat([int(line.strip()) for line in f]).T\n",
    "\n",
    "def read_probs(fname):\n",
    "    probs = dict()\n",
    "    for action in range(1, 5):\n",
    "        probs[action] = defaultdict(list)\n",
    "        name, ext = fname.split('.')\n",
    "        filename = name + str(action) + '.' + ext\n",
    "        with open(filename) as f:\n",
    "            for line in f:\n",
    "                content = line.split()\n",
    "                s_cur, s_next, p = int(content[0]), int(content[1]), float(content[2])\n",
    "                probs[action][s_cur].append((s_next, p))\n",
    "    return probs\n",
    "\n",
    "rewards = read_rewards(REWARD_FILE)\n",
    "probs = read_probs(P_FILE)\n",
    "\n",
    "# Step 2: Initialize variables\n",
    "gamma = 0.99\n",
    "states = range(1, 82)\n",
    "actions = range(1, 5)\n",
    "policy = {state: np.random.randint(low = 1, high = 5) for state in states}\n",
    "\n",
    "# Step 3: Start policy iteration\n",
    "\n",
    "def evaluate_values():\n",
    "    M = np.eye(len(states))\n",
    "    for state in states:\n",
    "        action = policy[state]\n",
    "        for s_next, p in probs[action][state]:\n",
    "            M[state - 1, s_next - 1] -= gamma * p\n",
    "    return np.linalg.solve(M, rewards)\n",
    "\n",
    "# values = np.mat() : MINUS 1\n",
    "# Evaluate Values\n",
    "def q_sa(state, action, values):\n",
    "    reward = 0\n",
    "    for s_next, p in probs[action][state]:\n",
    "        reward += p * values[s_next - 1]\n",
    "    return rewards[state - 1] + gamma * reward\n",
    "\n",
    "# Greedy update policy\n",
    "def update_policy(values):\n",
    "    is_updated = False\n",
    "    policy_new = {state: None for state in states}\n",
    "    for state in states:\n",
    "        q_max, action_best = float('-inf'), None\n",
    "        for action in actions:\n",
    "            q_sa_value = q_sa(state, action, values)\n",
    "            if q_max < q_sa_value:\n",
    "                q_max, action_best = q_sa_value, action\n",
    "        policy_new[state] = action_best\n",
    "        if action_best != policy[state]:\n",
    "            is_updated = True\n",
    "    return is_updated, policy_new\n",
    "\n",
    "is_updated = True\n",
    "iter = 0\n",
    "while is_updated:\n",
    "    values = evaluate_values()\n",
    "    is_updated, policy = update_policy(values)\n",
    "    iter += 1\n",
    "print(\"Iteration = {}\".format(iter))\n",
    "\n",
    "best_value = evaluate_values().reshape((9, 9)).T\n",
    "print(best_value)\n",
    "# np.savetxt('bestvalue.txt', best_value, fmt='%g')\n",
    "best_policy = np.array([action for _, action in sorted(list(policy.items()))]).reshape((9, 9)).T\n",
    "print(best_policy)\n",
    "# np.savetxt('bestpolicy.txt', best_policy, fmt='%g')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
